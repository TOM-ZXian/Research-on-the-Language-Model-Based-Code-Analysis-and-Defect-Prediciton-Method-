# Research-on-the-Language-Model-Based-Code-Analysis-and-Defect-Prediciton-Method-(Ph.D dissertation by Xian Zhang 2019)

Xian Zhang. Research on the Language Model Based Code Analysis and Defect Prediciton Method. Wuhan, China: Naval University of Engineering, 2019.
张献. 基于语言模型的代码分析及缺陷预测方法研究[D]. 武汉: 海Jun工程大学, 2019

KEYWORDS: software analysis, language model, code naturalness, defect prediction, slice granularity, software measurement, cross-entropy, deep learning

ABSTRACT
With the rapid increase of software size and complexity, software quality problems are becoming more and more serious. Software analysis is an effective method to guarantee software quality and plays an important role in predicting, detecting and confirming software defects. In recent years, code resources have rapidly accumulated and expanded, driving the methods of natural language processing, machine learning and other fields to analyze software. 

Language models are derived from statistical natural language processing. It aims to capture language occurrence rules and usage patterns through corpus, and to estimate the joint probability distribution of language sequences. With this capability, language models can mine software corpus and measure the naturalness of code. Generally, defective codes are more "unnatural". Code naturalness is characterized by the cross-entropy (CE) value measured by language model. The higher the CE value, the lower the probability of occurrence of code and relatively the greater the probability of being defective. In order to improve software quality, some works have been done in recent years to explore defect prediction methods based on code naturalness, but they still have the following shortcomings: traditional language models used have inherent data sparsity, curse of dimensionality and other issues; traditional language models only analyze code data, and the explicit information related to tasks is not effectively utilized; code-naturalness feature is measured only from a single perspective and owns limited discrimination; common coarse-grained defect prediction is difficult to assist in defect finding and location.

Aiming at the above problems, this paper first studies language models based on deep learning technology for software code, then studies code naturalness measurement and defect prediction methods based on such models. The main work includes:

Firstly, an additional information-guided neural language model for software code (NLMCode-AInfo) is proposed. Using the advantages of deep learning methods in pattern learning, a neural language model (NLMCode) based on word-embedding technology and stacked long and short-term memory network is constructed, which overcomes the curse of dimensionality and other issues of traditional language model. On this basis, aiming at the insufficient utilization of non-code information in existing models, an improved language model NLMCode-AInfo is designed to enhance model learning and decision-making ability by introducing task-related explicit information. In code suggestion task, the model integrates the project information represented in word-embedding form into the neural network, which improves the pertinence of data learning and word prediction ability. In the validation experiment on a real corpus with 2.03M lines of code, the improved model's suggestion accuracy rate (MRR index) is 3.6%~24.7% higher than the comparied methods. 

Secondly, a neural language model based code measurement and defect predictor method (NLMDePor) is proposed. This method uses the neural language model NLMCode to automatically calculate the CE values of given software modules, and then combines the generated code-naturalness CE as a new class of metrics with traditional software metrics together to complete more accurate defect prediction. The new metrics introduced by NLMDePor include CE-token obtained by the language model for the code token sequences and CE-AST obtained from the abstract syntax tree node sequences, which characterize code’s naturalness from different perspectives. To verify the effectiveness of the method, 12 benchmark defect datasets and 20 widely used code metrics were selected. The experimental results show that the discriminantive power of the two CE metrics generated by NLMDePor is better than nearly 50% of the traditional metrics, and they can bring 8.43%~9.58% relative increase in F1-score performance of the prediction models based on the traditional metrics. This shows that the CE metrics are complementary to the traditional metrics.

Thirdly, a bidirectional weighted neural language model based code measurement and defect predictor method (Bi-W-NLMDePor) is proposed. This method first constructs a bidirectional weighted language model for the problems that existing models only measure code in the forward direction and fail to utilize defect information. Based on the built NLMCode-AInfo, this imporved model uses software quality-type information to weigh samples, which enhances/inhibites the learning strength of the model on the clean/defective code, and improves the defect discrimination of CE-type metrics. On the other hand, it also realizes the bidirectional learning of input sequences, where the two improved metrics (M-CE and M-CE-Inv) acquired by the model can more comprehensively characterize the naturalness of code. Based on this model, Bi-W-NLMDePor method shares the label information of training samples in the defect-prediction phase to the language-model-learning phase, overcomes the shortcomings of the original method in isolated processing, and then combines the naturalness features meaursed by the language model with traditional software metrics together to feed prediction models, which improves the defect prediction performance.

Fourthly, software metric design and application of defect prediction methods are implemented on the statement-oriented slice-level granularity. The software module in such prediction granularity is composed of one or more statement-oriented program slices, in which slicing criteria are designed according to specific defect types, such as vulnerability-sensitive function calls. Four metrics are designed for the slice-level defect prediction, including code naturalness features (M-CE and M-CE-Inv) and code size features (lines of code and code token size). Using Bi-W-NLMDePor method, defect prediction applications and method validation are carried out on the slice-level datasets with two types of defect (buffer error and resource management error). The experimental results show that the Bi-W-NLMDePor method can effectively identify these two types of defects, with the F1-socre 9.7%~46.6% higher than the traditional defect prediction methods, 39.5%~66.6% higher than the pattern-based defect detection methods, and 52.5%~61.1% higher than the advanced code similarity-based defect detection methods. According to statistics, the average code lines of the slice-level modules used is 9.16 (far less than the commonly used file-level modules), which effectively compensates for the difficulty of locating buggy code areas and the high cost of code review in coarse-grained defect prediction.

KEYWORDS: software analysis, language model, code naturalness, defect prediction, slice granularity, software measurement, cross-entropy, deep learning

摘  要
随着软件规模与复杂性的快速增长，软件质量问题日渐突出。软件分析是保障软件质量的一种有效方法，在预测、发现及确认软件缺陷方面发挥了重要作用。近年来，代码资源迅速积累并日益扩增，为采用自然语言处理、机器学习等方法分析软件提供了数据资源。
语言模型源自统计自然语言处理，旨在通过语料库学习捕获语言发生规律和使用模式，实现对语言序列联合概率分布的估计。借助这种能力，语言模型可以挖掘软件语料库，衡量代码的自然性，通常有缺陷的代码更加“不自然”。代码自然性由语言模型度量的代码交叉熵（Cross-Entropy，CE）值表征。若CE值越高，则表明代码的发生概率越低，相对而言有缺陷的可能性越大。为提高软件质量，近几年已有一些工作对基于代码自然性的缺陷预测方法进行了探索，但它们还存在以下不足：使用的传统语言模型存在固有的数据稀疏、维数灾难等问题；传统语言模型仅对代码数据分析，而与任务相关的显性信息未被有效利用；代码自然性特征度量角度单一且缺陷判别力有限；常见的粗粒度缺陷预测难以辅助缺陷查找与定位等。
针对上述问题，本文首先面向软件代码研究基于深度学习的语言模型建模技术，然后研究基于此类模型的代码自然性度量及缺陷预测方法。主要工作包括：
第一，提出了一种面向软件代码的附加信息引导神经语言模型（NLMCode-AInfo）。首先利用深度学习方法在模式学习方面的优势，构建了基于词嵌入技术和栈式长短期记忆网络的神经语言模型（NLMCode），克服了传统语言模型的维数灾难等问题。在此基础上，针对现有模型对非代码信息利用不充分的问题，设计了改进的语言模型NLMCode-AInfo，旨在通过引入任务相关显性信息，增强模型学习和决策能力。在代码提示任务中，该模型将词嵌入表示的项目信息融入神经网络，提高了数据学习的针对性和词汇预测能力。在验证实验中，针对203万行真实代码语料库，改进模型的提示准确率（MRR指标）较多种对比方法有3.6%~24.7%的提高。
第二，提出了一种基于神经语言模型的代码度量及缺陷预测方法（NLMDePor）。该方法利用构建的神经语言模型NLMCode来自动计算给定软件模块的CE值，然后将生成的代码自然性特征CE作为一类新度量元与传统软件度量元相结合，一同完成更准确的缺陷预测。NLMDePor引入的新度量元包括语言模型针对代码词汇序列得到的CE-token和针对抽象语法树节点序列得到的CE-AST，它们从不同角度刻画了代码的自然性。为验证方法的有效性，选取了12个基准缺陷数据集和20个常用代码度量元。实验结果表明，NLMDePor生成的两种CE度量元的判别力均优于近50%的传统度量元，并可为基于传统度量元的预测模型性能（F1指标）带来8.4%~9.5%的相对增长，说明了CE度量元与传统度量元具有互补性。
第三，提出了一种基于双向加权神经语言模型的代码度量及缺陷预测方法（Bi-W-NLMDePor）。该方法首先针对现有语言模型仅对代码进行单向度量和未能利用缺陷信息的问题构建了双向加权语言模型。该模型一方面以NLMCode-AInfo为基础，利用软件质量信息对样本加权，增强/抑制了模型对无/有缺陷代码的学习强度，提升了CE类度量元的缺陷判别力；另一方面还实现了对输入序列的正逆双向学习，得到的两种改进度量元（M-CE和M-CE-Inv）更全面地刻画了代码的自然性。基于此模型，Bi-W-NLMDePor方法将缺陷预测阶段训练样本的标签信息共享给语言模型学习阶段，克服了原有方法分阶段处理的不足，然后将语言模型度量的自然性特征与传统软件度量元相结合一同输入给预测模型，改善了缺陷预测性能。
第四，在面向语句的切片级粒度上实现了度量元设计和缺陷预测方法应用。这一预测粒度分析的软件模块是由一个或多个面向语句的程序切片构成，其中切片准则依据具体缺陷类型设计，如漏洞敏感函数调用。针对切片粒度缺陷预测，设计了4种度量元，包括代码自然性特征M-CE和M-CE-Inv及规模特征代码行和代码词汇规模。利用Bi-W-NLMDePor方法，在两类切片粒度缺陷（缓冲区错误和资源管理错误）数据集上进行了缺陷预测应用及方法验证。实验结果表明，本文Bi-W-NLMDePor方法可以有效识别这两类缺陷，同时较传统缺陷预测方法F1指标提升9.7%~46.6%，较基于模式的缺陷检测方法提升39.5%~66.6%，较先进的基于代码相似性的缺陷检测方法提升52.5%~61.1%。经统计，使用的切片粒度模块的平均代码行为9.16（远小于常用的文件粒度模块），有效弥补了粗粒度缺陷预测难以聚焦缺陷区域、代码审查成本高的不足。

关键词：软件分析  语言模型  代码自然性  缺陷预测  切片粒度  软件度量  交叉熵  深度学习
